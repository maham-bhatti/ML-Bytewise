{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4990 images belonging to 5 classes.\n",
      "Found 4990 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 0.4372 - loss: 9.6096 - val_accuracy: 0.5385 - val_loss: 1.1440\n",
      "Epoch 2/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 2s/step - accuracy: 0.5243 - loss: 1.1692 - val_accuracy: 0.5415 - val_loss: 1.1557\n",
      "Epoch 3/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 2s/step - accuracy: 0.5464 - loss: 1.1188 - val_accuracy: 0.5315 - val_loss: 1.0915\n",
      "Epoch 4/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 2s/step - accuracy: 0.5249 - loss: 1.1146 - val_accuracy: 0.5491 - val_loss: 1.1011\n",
      "Epoch 5/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 2s/step - accuracy: 0.5303 - loss: 1.0883 - val_accuracy: 0.5489 - val_loss: 1.0917\n",
      "Epoch 6/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3094s\u001b[0m 20s/step - accuracy: 0.5472 - loss: 1.0435 - val_accuracy: 0.5303 - val_loss: 1.0459\n",
      "Epoch 7/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 2s/step - accuracy: 0.5469 - loss: 1.0520 - val_accuracy: 0.5218 - val_loss: 1.0457\n",
      "Epoch 8/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 2s/step - accuracy: 0.5272 - loss: 1.0396 - val_accuracy: 0.5401 - val_loss: 1.0406\n",
      "Epoch 9/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 2s/step - accuracy: 0.5469 - loss: 1.0039 - val_accuracy: 0.5355 - val_loss: 0.9789\n",
      "Epoch 10/10\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 2s/step - accuracy: 0.5285 - loss: 1.0111 - val_accuracy: 0.5216 - val_loss: 1.0194\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m hybrid_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Train the hybrid model\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Evaluate both models\u001b[39;00m\n\u001b[0;32m     72\u001b[0m simple_model_score \u001b[38;5;241m=\u001b[39m simple_model\u001b[38;5;241m.\u001b[39mevaluate(validation_generator)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\input_spec.py:186\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mallow_last_axis_squeeze:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m spec\u001b[38;5;241m.\u001b[39mmax_ndim:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 512)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Embedding, Attention\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "# Training and validation directories\n",
    "train_dir = \"C:/Users/Lenovo/Desktop/Bytewise_Project/train\"\n",
    "validation_dir = \"C:/Users/Lenovo/Desktop/Bytewise_Project/train\"\n",
    "\n",
    "# no. of classes are 5\n",
    "num_classes = 5 \n",
    "\n",
    "# Image dimensions\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# Data augmentation parameters\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Simple CNN model\n",
    "simple_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_width, image_height, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "simple_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train of simple model\n",
    "simple_model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# The hybrid CNN-LSTM model\n",
    "hybrid_model = Sequential([\n",
    "    VGG16(include_top=False, weights='imagenet', input_shape=(image_width, image_height, 3)),\n",
    "    GlobalAveragePooling2D(),\n",
    "    LSTM(64),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "hybrid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the hybrid model\n",
    "hybrid_model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Evaluate both models\n",
    "simple_model_score = simple_model.evaluate(validation_generator)\n",
    "hybrid_model_score = hybrid_model.evaluate(validation_generator)\n",
    "\n",
    "print(\"Simple Accuracy of Model:\", simple_model_score[1])\n",
    "print(\"Hybrid Accuracy of Model:\", hybrid_model_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPORT:\n",
    "\n",
    "I take the dataset from the Kaggle.\n",
    "\n",
    "The dataset name is Flowers \"Five Classes\"\n",
    "\n",
    "The five classes consists of daisy, rose,dandelion, rose, sunflower, and tulip flowers images.\n",
    "\n",
    "The dataset consits of classes of flowers images.\n",
    "\n",
    "Used for image classification.\n",
    "\n",
    "I faced the issue of how to load the dataset image from one directory that have sub-directories.\n",
    "\n",
    "So, I take the help from Google and remove this difficulty.\n",
    "\n",
    "Model Architecture:\n",
    "Used two models Simple model & Hybrid CNN/LSTM Model.\n",
    "\n",
    "In simple model include the no.of layers, activation functions, & filters.\n",
    "\n",
    "In Hybrid I integrate the CNN & LSTM Layers. Evaluate the metrices for hybrid model.\n",
    "\n",
    "The images have high resolution & pixels.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
